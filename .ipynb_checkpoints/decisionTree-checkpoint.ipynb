{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa717a3-4eec-4560-9abe-be976fd8735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='xsede_revised.csv', type=<function <lambda> at 0x1289ab430>, choices=None, help='dataset file name or list of dataset files separated by comma.', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--dataset\",\n",
    "    type=lambda expr: [\n",
    "        os.path.abspath(\"../DataFiles/{}\".format(r)) for r in expr.split(\",\")],\n",
    "    default=\"xsede_revised.csv\",\n",
    "    help=\"dataset file name or list of dataset files separated by comma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679d79ee-e7ca-4c81-a06c-92851d8f5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_file_location is a \"list\" of considered dataset files\n",
    "#function takes a list and return a dataframe created from provided locations.\n",
    "#capable of creating a single dataframe for multiple data file locations\n",
    "\n",
    "def load_dataset_from_file(dataset_file_location):\n",
    "    result_df=pd.read_csv(dataset_file_location[0])\n",
    "    if len(dataset_file_location)>1:\n",
    "        for i in range(1,len(dataset_file_location)):\n",
    "            temp_df=pd.read_csv(dataset_file_location[i])\n",
    "            result_df=pd.concat([result_df, temp_df], axis=0, join='inner')\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def extractRequiredColumn(df,requiredFields):\n",
    "    return df[df.columns[df.columns.isin(requiredFields)]]\n",
    "\n",
    "def NormalizeData(df):\n",
    "    df_no_NA=df.replace(to_replace=\"na\",value= 1).astype(float) # line replace any 'na' in the dataset with 0\n",
    "    return df_no_NA/df_no_NA.max()\n",
    "\n",
    "def set_labels(dataFrame,LabelName):\n",
    "    return np.array(dataFrame[LabelName])\n",
    "\n",
    "def set_features(dataFrame,LabelName,dropFeatureList):\n",
    "    dataFrame= dataFrame.drop(LabelName, axis = 1)\n",
    "    for i in dropFeatureList:\n",
    "        dataFrame= dataFrame.drop(i, axis = 1)\n",
    "    feature_list=list(dataFrame.columns)\n",
    "    return np.array(dataFrame),feature_list\n",
    "\n",
    "class ReadFile:\n",
    "    def __init__(self,\n",
    "                dataset_file_location,requiredFields):\n",
    "\n",
    "        #dataset_file_location is a \"list\" of considered dataset files\n",
    "        self.dataset=load_dataset_from_file(dataset_file_location)\n",
    "        self.requiredData=extractRequiredColumn(self.dataset,requiredFields)\n",
    "        self.requiredNormalizedData=NormalizeData(self.requiredData)\n",
    "\n",
    "class pd2Array:\n",
    "    def __init__(self,dataFrame,LabelName,dropFeatureList):\n",
    "\n",
    "        self.labels=set_labels(dataFrame,LabelName)\n",
    "        self.features,self.feature_list=set_features(dataFrame,LabelName,dropFeatureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc49672e-ddbe-4e41-a1d7-9344ec261129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/jamil/Library/Jupyter/runtime/kernel-fd63acfa-c27e-42ae-a58c-f3fc62f1e6e1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamil/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    #requiredFields is a list of the fields containing attributes and the output\n",
    "    requiredFields=['FileSize', 'FileCount',\n",
    "   'Bandwidth', 'RTT', 'BufferSize', 'Parallelism', 'Concurrency',\n",
    "   'Pipelining', 'Throughput']\n",
    "    LabelName='Throughput'\n",
    "    dropFeatureList=['Parallelism', 'Concurrency',\n",
    "   'Pipelining']\n",
    "   # fileData is an object of ReadFile class\n",
    "\n",
    "    fileData=ReadFile(args.dataset,requiredFields)\n",
    "\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #       # more options can be specified also\n",
    "    #     print(fileData.requiredNormalizedData)\n",
    "    # print(fileData.requiredNormalizedData['FileSize'])\n",
    "\n",
    "\n",
    "    # print(fileData.requiredNormalizedData.describe())\n",
    "    processedData=pd2Array(fileData.requiredNormalizedData,LabelName,dropFeatureList)\n",
    "    # print(processedData.labels,len(processedData.labels)) # processedData.labels is np.array\n",
    "    # print(processedData.features,processedData.features.shape) #processedData.features are dataFrame\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(processedData.features, processedData.labels, test_size = 0.25, random_state = 42)\n",
    "    # print('Training Features Shape:', train_features.shape)\n",
    "    # print('Training Labels Shape:', train_labels.shape)\n",
    "    # print('Testing Features Shape:', test_features.shape)\n",
    "    # print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "    rf.fit(train_features, train_labels)\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_features)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "    # Print out the mean absolute error (mae)\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / test_labels)\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    tree = rf.estimators_[5]\n",
    "    export_graphviz(tree, out_file = 'tree.dot', feature_names = processedData.feature_list, rounded = True, precision = 1)\n",
    "    # Use dot file to create a graph\n",
    "    (graph, ) = pydot.graph_from_dot_file('./tree.dot')\n",
    "    # Write graph to a png file\n",
    "    graph.write_png('tree.png')\n",
    "\n",
    "        # Limit depth of tree to 3 levels\n",
    "    rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "    rf_small.fit(train_features, train_labels)\n",
    "    # Extract the small tree\n",
    "    tree_small = rf_small.estimators_[5]\n",
    "    # Save the tree as a png image\n",
    "    export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = processedData.feature_list, rounded = True, precision = 1)\n",
    "    (graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "    graph.write_png('small_tree.png')\n",
    "\n",
    "    # Get numerical feature importances\n",
    "    importances = list(rf.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(processedData.feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Print out the feature and importances\n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "\n",
    "    instance=np.array([i for i in range(2831)])\n",
    "\n",
    "    true_data = pd.DataFrame(data = {'Instance': instance, 'actual': test_labels})\n",
    "\n",
    "    predictions_data = pd.DataFrame(data = {'Instance': instance, 'prediction': predictions})\n",
    "    # Plot the actual values\n",
    "    plt.plot(true_data['Instance'], true_data['actual'], 'b-', label = 'actual')\n",
    "    # Plot the predicted values\n",
    "    plt.plot(predictions_data['Instance'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
    "    plt.xticks(rotation = '60');\n",
    "    plt.legend()\n",
    "    # Graph labels\n",
    "    plt.xlabel('Instance'); plt.ylabel('Throughput'); plt.title('Actual and Predicted Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dddb7b79-2ddc-4f89-ab49-f369ae051a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8738f9943dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#requiredFields is a list of the fields containing attributes and the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     requiredFields=['FileSize', 'FileCount',\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m'Bandwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RTT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BufferSize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Parallelism'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Concurrency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51669c73-99c2-4805-a070-f038c4a61e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
